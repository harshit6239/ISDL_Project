{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn9YdvHTwp02"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset, DatasetDict\n",
        "import evaluate\n",
        "import wandb\n",
        "import pandas as pd\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Set seed for reproducibility\n",
        "def set_seed(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Kaggle API authentication (Use environment variables securely)\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "# Step 1: Download Dataset\n",
        "dataset_path = \"patzshane/football-commentary-data-set-college-and-nfl\"\n",
        "local_path = \"./football_commentary_dataset\"\n",
        "os.makedirs(local_path, exist_ok=True)\n",
        "api.dataset_download_files(dataset_path, path=local_path, unzip=True)\n",
        "\n",
        "# Load Dataset from Local Path\n",
        "dataset_file = os.path.join(local_path, \"data.csv\")  # Adjust if needed\n",
        "data = pd.read_csv(dataset_file)\n",
        "print(data.head())\n",
        "\n",
        "# Step 2: Convert Dataset to Hugging Face Format\n",
        "raw_datasets = load_dataset(\"csv\", data_files={\"train\": dataset_file})\n",
        "split_datasets = raw_datasets[\"train\"].train_test_split(test_size=0.1)  # 90% train, 10% validation\n",
        "\n",
        "# Step 3: Initialize Pre-Trained Model and Tokenizer\n",
        "model_name = \"mistralai/Mistral-7B-Instruct\"  # Replaced incorrect Gemini model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Add padding token if missing\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    model.config.pad_token_id = tokenizer.pad_token_id  # Ensure model recognizes pad token\n",
        "\n",
        "# Step 4: Tokenize Dataset with Dynamic Padding\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "tokenized_datasets = split_datasets.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # We are not using masked language modeling\n",
        ")\n",
        "\n",
        "# Step 5: Set Up Training Arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./autogen_results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=500,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    gradient_accumulation_steps=2,\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "# Step 6: Initialize Evaluation Metrics\n",
        "perplexity = evaluate.load(\"perplexity\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_advanced_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = torch.argmax(torch.tensor(logits), dim=-1).tolist()\n",
        "    \n",
        "    predictions_texts = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    references_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    bleu_score = bleu.compute(predictions=predictions_texts, references=references_texts)\n",
        "    rouge_score = rouge.compute(predictions=predictions_texts, references=references_texts)\n",
        "    perplexity_score = perplexity.compute(predictions=logits, references=labels)\n",
        "    \n",
        "    return {\n",
        "        \"perplexity\": perplexity_score[\"perplexity\"],\n",
        "        \"bleu\": bleu_score[\"bleu\"],\n",
        "        \"rouge\": rouge_score[\"rouge1\"],\n",
        "    }\n",
        "\n",
        "# Step 7: Initialize Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],  # Proper validation set\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_advanced_metrics,\n",
        ")\n",
        "\n",
        "# Step 8: Fine-Tune Model\n",
        "trainer.train()\n",
        "\n",
        "# Step 9: Evaluate Model\n",
        "results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", results)\n",
        "\n",
        "# Step 10: Save Fine-Tuned Model\n",
        "output_dir = \"./finetuned_model\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"Model saved to {output_dir}\")\n",
        "\n",
        "# Step 11: Generate Text with Fine-Tuned Model\n",
        "prompt = \"In the field of artificial intelligence, one of the most exciting developments is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "outputs = model.generate(\n",
        "    inputs[\"input_ids\"],\n",
        "    max_length=50,\n",
        "    num_return_sequences=1,\n",
        "    temperature=0.8,\n",
        "    top_k=50,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.2\n",
        ")\n",
        "\n",
        "print(\"Generated Text:\")\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
